{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hOl5sDXo_C_C"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guntsvzz/CP_Project/blob/main/EEG_Stress_CNN_Unchunking_Cleaned_CrossEntropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kkVXPs1iOqI",
        "outputId": "c6570ea5-f33a-4b75-8a83-60f073be22a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.5.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1KZjrJfUrB_k1g8VUsohNpek_hvWNqGcr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3AnMbRCkb0E",
        "outputId": "c777ef56-e599-4987-877f-a50ecf02dee1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KZjrJfUrB_k1g8VUsohNpek_hvWNqGcr\n",
            "To: /content/Clean_signal.zip\n",
            "100% 113M/113M [00:00<00:00, 214MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/Clean_signal.zip'\n",
        "#!mv '/content/clean_exp16' 'Cleaned_signal'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY30dqh8kjSM",
        "outputId": "1495aad7-4385-43d7-a66c-3c03edf2ba6b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Clean_signal.zip\n",
            "replace __MACOSX/._Clean_signal? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: __MACOSX/._Clean_signal  \n",
            "  inflating: Clean_signal/PSS10 - Sheet1.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._PSS10 - Sheet1.csv  \n",
            "  inflating: Clean_signal/clean_exp08.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp08.csv  \n",
            "  inflating: Clean_signal/clean_exp09.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp09.csv  \n",
            "  inflating: Clean_signal/clean_exp01.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp01.csv  \n",
            "  inflating: Clean_signal/clean_exp15.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp15.csv  \n",
            "  inflating: Clean_signal/clean_exp14.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp14.csv  \n",
            "  inflating: Clean_signal/clean_exp16.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp16.csv  \n",
            "  inflating: Clean_signal/clean_exp02.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp02.csv  \n",
            "  inflating: Clean_signal/clean_exp03.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp03.csv  \n",
            "  inflating: Clean_signal/clean_exp17.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp17.csv  \n",
            "  inflating: Clean_signal/clean_exp13.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp13.csv  \n",
            "  inflating: Clean_signal/clean_exp07.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp07.csv  \n",
            "  inflating: Clean_signal/clean_exp06.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp06.csv  \n",
            "  inflating: Clean_signal/clean_exp12.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp12.csv  \n",
            "  inflating: Clean_signal/clean_exp04.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp04.csv  \n",
            "  inflating: Clean_signal/clean_exp10.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp10.csv  \n",
            "  inflating: Clean_signal/clean_exp11.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp11.csv  \n",
            "  inflating: Clean_signal/clean_exp05.csv  \n",
            "  inflating: __MACOSX/Clean_signal/._clean_exp05.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pip install mne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip0Qx71RiRak",
        "outputId": "ff0b6c24-ee46-4681-a0f5-2a29791174f6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mne) (4.64.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mne) (3.2.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne) (2.11.3)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.7/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.21.6)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.7.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mne) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.9.24)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mne) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mne) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install components"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GxiHrcZiRdm",
        "outputId": "c1cd3e62-e9a4-407d-ee23-e7686590f995"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: components in /usr/local/lib/python3.7/dist-packages (1.2.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import mne\n",
        "import pandas as pd\n",
        "pickle.format_version\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms  \n",
        "from torch.utils.data import DataLoader, Dataset  \n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# setting seed so that splitting process and training process can be reproduce\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY4wJP1-iRgP",
        "outputId": "400984f6-5430-4355-eb61-3c82754b7fe4"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f30f56739f0>"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL"
      ],
      "metadata": {
        "id": "7F96WFs9ihQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "main_path = os.getcwd()\n",
        "filename = os.listdir(main_path + '/Clean_signal/')"
      ],
      "metadata": {
        "id": "1aQoZm4jibEr"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = ['clean_exp01.csv',\n",
        " 'clean_exp02.csv',\n",
        " 'clean_exp03.csv',\n",
        " 'clean_exp04.csv',\n",
        " 'clean_exp05.csv',\n",
        " 'clean_exp06.csv',\n",
        " 'clean_exp07.csv',\n",
        " 'clean_exp08.csv',\n",
        " 'clean_exp09.csv',\n",
        " 'clean_exp10.csv',\n",
        " 'clean_exp11.csv',\n",
        " 'clean_exp12.csv',\n",
        " 'clean_exp13.csv',\n",
        " 'clean_exp14.csv',\n",
        " 'clean_exp15.csv',\n",
        " 'clean_exp16.csv',\n",
        " 'clean_exp17.csv']"
      ],
      "metadata": {
        "id": "Z49O-2H2k6p5"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_file_path = main_path + '/Clean_signal/'\n",
        "X_list = list()\n",
        "for file in filename:\n",
        "    file_path = all_file_path + file\n",
        "    df = pd.read_csv(file_path, index_col=None)\n",
        "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "    X_list.append(df) "
      ],
      "metadata": {
        "id": "7N78RJfiibHn"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "eGs1ADkribKx",
        "outputId": "60b49c6a-4899-4cef-d6de-79b3a1e6f261"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Fp1           Fp2            F7            F3            F4  \\\n",
              "0  1.558541e-13  1.202787e-12 -1.822815e-12 -6.666149e-13 -1.524659e-14   \n",
              "1 -2.495299e+02 -2.591266e+02 -2.392414e+02 -2.472481e+02 -1.342918e+02   \n",
              "2 -1.022146e+02 -1.071695e+02 -8.913196e+01 -9.320015e+01 -7.188064e+01   \n",
              "3 -1.894707e+02 -1.985657e+02 -1.666088e+02 -1.773347e+02 -1.206077e+02   \n",
              "4 -1.976600e+02 -2.088452e+02 -1.691085e+02 -1.829386e+02 -1.219805e+02   \n",
              "\n",
              "             F8            T3            C3            C4            T4  \\\n",
              "0 -9.656176e-14  1.778769e-13 -7.318365e-13 -3.218725e-14  6.776264e-15   \n",
              "1 -2.469675e+02 -2.368676e+02 -2.375381e+02  8.964020e+01  9.114929e+01   \n",
              "2 -9.669399e+01 -7.360927e+01 -7.934479e+01 -8.483305e+00 -3.966042e+00   \n",
              "3 -1.861381e+02 -1.566157e+02 -1.616160e+02  1.053098e+01  1.359837e+01   \n",
              "4 -1.981792e+02 -1.664133e+02 -1.711082e+02  5.430791e+00  5.540176e+00   \n",
              "\n",
              "             T5            P3            P4            T6            O1  \\\n",
              "0  2.463172e-12 -5.293956e-13  1.679243e-13  2.774456e-12 -3.303428e-13   \n",
              "1  9.105020e+01  7.684951e+01  1.383037e+02 -4.155527e+02 -3.083336e+02   \n",
              "2  6.921319e-02 -5.860820e+00  6.340924e+00 -1.006591e+02 -7.385641e+01   \n",
              "3  2.570865e+01  1.375228e+01  4.319836e+01 -2.383818e+02 -1.751361e+02   \n",
              "4  2.039321e+01  9.522058e+00  4.153111e+01 -2.713544e+02 -1.985949e+02   \n",
              "\n",
              "             O2  \n",
              "0  2.217109e-13  \n",
              "1 -1.145710e+02  \n",
              "2 -3.186257e+01  \n",
              "3 -7.472951e+01  \n",
              "4 -9.082392e+01  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c9f57d61-1e54-43e8-8227-771613e8a3b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fp1</th>\n",
              "      <th>Fp2</th>\n",
              "      <th>F7</th>\n",
              "      <th>F3</th>\n",
              "      <th>F4</th>\n",
              "      <th>F8</th>\n",
              "      <th>T3</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>T4</th>\n",
              "      <th>T5</th>\n",
              "      <th>P3</th>\n",
              "      <th>P4</th>\n",
              "      <th>T6</th>\n",
              "      <th>O1</th>\n",
              "      <th>O2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.558541e-13</td>\n",
              "      <td>1.202787e-12</td>\n",
              "      <td>-1.822815e-12</td>\n",
              "      <td>-6.666149e-13</td>\n",
              "      <td>-1.524659e-14</td>\n",
              "      <td>-9.656176e-14</td>\n",
              "      <td>1.778769e-13</td>\n",
              "      <td>-7.318365e-13</td>\n",
              "      <td>-3.218725e-14</td>\n",
              "      <td>6.776264e-15</td>\n",
              "      <td>2.463172e-12</td>\n",
              "      <td>-5.293956e-13</td>\n",
              "      <td>1.679243e-13</td>\n",
              "      <td>2.774456e-12</td>\n",
              "      <td>-3.303428e-13</td>\n",
              "      <td>2.217109e-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.495299e+02</td>\n",
              "      <td>-2.591266e+02</td>\n",
              "      <td>-2.392414e+02</td>\n",
              "      <td>-2.472481e+02</td>\n",
              "      <td>-1.342918e+02</td>\n",
              "      <td>-2.469675e+02</td>\n",
              "      <td>-2.368676e+02</td>\n",
              "      <td>-2.375381e+02</td>\n",
              "      <td>8.964020e+01</td>\n",
              "      <td>9.114929e+01</td>\n",
              "      <td>9.105020e+01</td>\n",
              "      <td>7.684951e+01</td>\n",
              "      <td>1.383037e+02</td>\n",
              "      <td>-4.155527e+02</td>\n",
              "      <td>-3.083336e+02</td>\n",
              "      <td>-1.145710e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.022146e+02</td>\n",
              "      <td>-1.071695e+02</td>\n",
              "      <td>-8.913196e+01</td>\n",
              "      <td>-9.320015e+01</td>\n",
              "      <td>-7.188064e+01</td>\n",
              "      <td>-9.669399e+01</td>\n",
              "      <td>-7.360927e+01</td>\n",
              "      <td>-7.934479e+01</td>\n",
              "      <td>-8.483305e+00</td>\n",
              "      <td>-3.966042e+00</td>\n",
              "      <td>6.921319e-02</td>\n",
              "      <td>-5.860820e+00</td>\n",
              "      <td>6.340924e+00</td>\n",
              "      <td>-1.006591e+02</td>\n",
              "      <td>-7.385641e+01</td>\n",
              "      <td>-3.186257e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.894707e+02</td>\n",
              "      <td>-1.985657e+02</td>\n",
              "      <td>-1.666088e+02</td>\n",
              "      <td>-1.773347e+02</td>\n",
              "      <td>-1.206077e+02</td>\n",
              "      <td>-1.861381e+02</td>\n",
              "      <td>-1.566157e+02</td>\n",
              "      <td>-1.616160e+02</td>\n",
              "      <td>1.053098e+01</td>\n",
              "      <td>1.359837e+01</td>\n",
              "      <td>2.570865e+01</td>\n",
              "      <td>1.375228e+01</td>\n",
              "      <td>4.319836e+01</td>\n",
              "      <td>-2.383818e+02</td>\n",
              "      <td>-1.751361e+02</td>\n",
              "      <td>-7.472951e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.976600e+02</td>\n",
              "      <td>-2.088452e+02</td>\n",
              "      <td>-1.691085e+02</td>\n",
              "      <td>-1.829386e+02</td>\n",
              "      <td>-1.219805e+02</td>\n",
              "      <td>-1.981792e+02</td>\n",
              "      <td>-1.664133e+02</td>\n",
              "      <td>-1.711082e+02</td>\n",
              "      <td>5.430791e+00</td>\n",
              "      <td>5.540176e+00</td>\n",
              "      <td>2.039321e+01</td>\n",
              "      <td>9.522058e+00</td>\n",
              "      <td>4.153111e+01</td>\n",
              "      <td>-2.713544e+02</td>\n",
              "      <td>-1.985949e+02</td>\n",
              "      <td>-9.082392e+01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9f57d61-1e54-43e8-8227-771613e8a3b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c9f57d61-1e54-43e8-8227-771613e8a3b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c9f57d61-1e54-43e8-8227-771613e8a3b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(X_list)\n",
        "X.shape \n",
        "#17 people \n",
        "#250 hz * 60 secs * 3 min\n",
        "#16 channels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbl_IKDCjbk5",
        "outputId": "479b1557-7da0-4855-b745-8cb056854ba1"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17, 45000, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape\n",
        "X = np.transpose(X, (0, 2, 1))\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFU5ZNkkinvr",
        "outputId": "fbe690a4-feac-4fce-85ee-88ec27a91657"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17, 16, 45000)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_path = '/content/Clean_signal/PSS10 - Sheet1.csv'\n",
        "df = pd.read_csv(y_path)\n",
        "y = np.array(df['label'])\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58PB1ZyqlLL9",
        "outputId": "f5b67477-b3e0-4dcf-f0af-986c987df20b"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data"
      ],
      "metadata": {
        "id": "GOBGZtryk_k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = False, stratify = None)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI2G9Bbsinyi",
        "outputId": "6ee6cf2c-cfa8-4da8-d6bd-f155b0a89011"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13, 16, 45000) (4, 16, 45000) (13,) (4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(X_test)"
      ],
      "metadata": {
        "id": "qYeicq65in1f"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=999)\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9FqFv4sni7e",
        "outputId": "7d62362a-f66d-4f02-f0ae-7115619040c1"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11, 16, 45000) (2, 16, 45000) (11,) (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#scale transform each channel independently\n",
        "scalers = {}\n",
        "for i in range(X_train.shape[2]):\n",
        "    scalers[i] = MinMaxScaler(feature_range=(-1, 1))\n",
        "    X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i]) \n",
        "\n",
        "for i in range(X_val.shape[2]):\n",
        "    X_val[:, :, i]   = scalers[i].transform(X_val[:, :, i])     \n",
        "    \n",
        "for i in range(X_test.shape[2]):\n",
        "    X_test[:, :, i]  = scalers[i].transform(X_test[:, :, i]) "
      ],
      "metadata": {
        "id": "S8pWHAGeni_f"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.mean(), X_val.mean(), X_test.mean())\n",
        "print(X_train.min(), X_val.min(), X_test.min())\n",
        "print(X_train.max(), X_val.max(), X_test.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLOO4OpynjDC",
        "outputId": "317b9643-0422-4dcd-e624-19cb6c6e32a6"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.0035532945742628333 -0.005295317732894995 -0.0031302117490915734\n",
            "-1.0 -418.85970347464144 -2797.1708453940587\n",
            "1.0000000000000004 749.4431360181195 2064.2315527719506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Batch size and train,test, val loader"
      ],
      "metadata": {
        "id": "RLKlKqBWno9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.tensor(X_train).to(torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train).to(torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val).to(torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val).to(torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test).to(torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test).to(torch.float32)\n",
        "\n",
        "# Cast data to dataloader for more convenience\n",
        "training_set = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "testing_set = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "validation_set = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "train_batch_size = 32\n",
        "val_batch_size = len(validation_set)\n",
        "test_batch_size = len(testing_set)\n",
        "\n",
        "train_loader = DataLoader(training_set, train_batch_size, shuffle=True)\n",
        "val_loader = DataLoader(validation_set, val_batch_size, shuffle=True)\n",
        "test_loader = DataLoader(testing_set, test_batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Y1enUccbhXGK"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data, label in train_loader:\n",
        "  print(data)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEJ3VJt3huYw",
        "outputId": "8f145c22-624b-4743-8df2-c6b05faa8a6f"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.2625,  1.0000,  0.5220,  ..., -0.1055,  0.4833, -1.0000],\n",
            "         [-0.1846,  1.0000,  0.8369,  ...,  0.6823,  0.9280,  0.0450],\n",
            "         [-0.6639,  1.0000,  0.5339,  ...,  0.1186,  0.7721,  0.4254],\n",
            "         ...,\n",
            "         [ 0.5534, -0.8758, -1.0000,  ..., -0.7690, -0.9071,  0.5299],\n",
            "         [ 0.6211, -0.7073, -1.0000,  ...,  0.1866, -0.2887,  0.1363],\n",
            "         [-1.0000,  0.4953, -0.4354,  ...,  0.8192,  0.1606, -1.0000]],\n",
            "\n",
            "        [[ 0.2994, -0.1263,  0.7198,  ..., -0.3135, -0.2813, -0.9733],\n",
            "         [-0.0283,  0.9211,  0.8718,  ...,  0.6713,  0.8581, -0.9717],\n",
            "         [-1.0000, -0.0266,  0.6665,  ...,  0.0394,  0.1550, -1.0000],\n",
            "         ...,\n",
            "         [ 1.0000, -0.8192, -0.7463,  ..., -0.9722, -0.9601,  1.0000],\n",
            "         [-1.0000, -0.8281, -0.7549,  ..., -1.0000, -0.8562, -0.8752],\n",
            "         [-0.5561,  0.2080, -0.3849,  ...,  0.4928,  0.0105,  0.7527]],\n",
            "\n",
            "        [[ 0.5755,  0.8754,  0.7470,  ...,  0.1080,  0.4361,  0.9391],\n",
            "         [-0.3997,  0.9955,  0.8537,  ...,  0.7921,  0.9400, -0.4045],\n",
            "         [ 1.0000,  0.8592,  0.7077,  ...,  0.2499,  0.6919,  0.6833],\n",
            "         ...,\n",
            "         [ 0.5082,  0.8885,  0.1780,  ...,  0.1747,  0.4627,  0.4576],\n",
            "         [ 0.9204, -0.8709, -0.7902,  ..., -0.4588, -1.0000,  0.1688],\n",
            "         [ 0.2527,  1.0000, -0.0767,  ...,  0.9433,  1.0000,  0.6136]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.4288, -0.2783,  0.5553,  ..., -0.7431, -0.2954, -0.6487],\n",
            "         [ 0.2392,  0.9222,  0.8314,  ...,  0.6884,  0.8668, -0.8444],\n",
            "         [-0.3856, -0.2737,  0.4790,  ..., -0.2838,  0.0564,  0.7432],\n",
            "         ...,\n",
            "         [ 0.4443, -0.8626, -0.6387,  ..., -0.9919, -0.9627,  0.4064],\n",
            "         [ 0.4864, -0.8789, -0.6865,  ..., -0.7044, -0.9055, -0.3003],\n",
            "         [-0.2640,  0.1913, -0.1873,  ...,  0.5611, -0.0199,  0.7895]],\n",
            "\n",
            "        [[ 0.2968,  0.3130,  0.7292,  ..., -1.0000, -1.0000, -0.7500],\n",
            "         [ 0.2077,  0.9244,  0.8293,  ...,  0.6008,  0.8237, -1.0000],\n",
            "         [-0.0490,  0.7991,  0.7552,  ..., -1.0000, -1.0000,  0.4394],\n",
            "         ...,\n",
            "         [-0.0447, -1.0000, -0.6010,  ..., -0.9183, -0.8995, -1.0000],\n",
            "         [ 1.0000, -1.0000, -0.6260,  ..., -0.3759, -0.5769,  1.0000],\n",
            "         [ 1.0000, -0.0737,  0.0366,  ...,  0.7941,  0.1552,  0.2712]],\n",
            "\n",
            "        [[-1.0000, -0.1040,  0.7342,  ...,  0.0077, -0.0924,  1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000,  1.0000],\n",
            "         [-0.3384,  0.0125,  0.7038,  ...,  0.1758,  0.1889,  0.0375],\n",
            "         ...,\n",
            "         [-0.0527,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  0.5849],\n",
            "         [ 0.4439, -0.9257, -0.8425,  ..., -0.6804, -0.8012, -0.1237],\n",
            "         [-0.3389, -0.6549, -0.7661,  ...,  0.3446, -0.2156,  0.9060]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToOmNlhsPSG6",
        "outputId": "0655d6e4-7dc5-4e4d-9fe8-447d3b896e02"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11, 16, 45000])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVMFMBwAjImF",
        "outputId": "fdc99c42-b601-4200-dc5d-ad746880e404"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11, 16, 45000])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "HaiKzvgCjc3_"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN model"
      ],
      "metadata": {
        "id": "tMV89z27_HmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class eegConv1d(nn.Module):\n",
        "    def __init__(self, input_size = 16, out_size=2):\n",
        "        super().__init__()\n",
        "        self.c1 = nn.Conv1d(input_size, 50, kernel_size = 3)\n",
        "        self.c2 = nn.Conv1d(50, 30, kernel_size = 3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.maxpool1d = nn.MaxPool1d(2,2)\n",
        "        self.linear = nn.Linear(30 * 22497, out_size) #taking the last hidden state\n",
        "        \n",
        "    def forward(self, seq):\n",
        "        #convo layer 8 -> 50 -> 30\n",
        "        #seq shape: (11, 50, 45000)\n",
        "        out = self.c1(seq)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        # out shape: (11, 50, 44998)\n",
        "        out = self.maxpool1d(out)\n",
        "        # out shape: (11, 50, 22499)\n",
        "        out = self.c2(out)\n",
        "        out = self.relu(out)\n",
        "        # out shape: (11, 30, 22496)\n",
        "        out = out.reshape(seq.size(0), -1)\n",
        "        #out shape: (30, 30*22496)\n",
        "        out = self.linear(out)\n",
        "        #out shape: (30*22496, 2)\n",
        "        return out"
      ],
      "metadata": {
        "id": "U0bk5XjGjoMk"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "TIOLWwwXpMpw"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(999999)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = eegConv1d(input_size=16).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#Good for finding Likelihood\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
      ],
      "metadata": {
        "id": "kpdAQllGD1tf"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "model.train()\n",
        "\n",
        "train_losses = []\n",
        "train_accs   = []\n",
        "valid_losses = []\n",
        "valid_accs   = []\n",
        "\n",
        "\n",
        "#print(f\"Training {type(model).__name__}\")\n",
        "\n",
        "for i in range(epochs):\n",
        "    train_total = 0\n",
        "    train_correct = 0\n",
        "    val_total   = 0  \n",
        "    val_correct = 0\n",
        "    train_acc   = 0\n",
        "    val_acc     = 0\n",
        "    \n",
        "    for X_train, y_train in train_loader:\n",
        "    \n",
        "        start_time = time.time()\n",
        "        \n",
        "        X_train = X_train.float().to(device)\n",
        "        y_train = y_train.type(torch.LongTensor).to(device)\n",
        "\n",
        "        #print(X_train.shape, X_train.dtype)\n",
        "\n",
        "        yhat_train = model(X_train)\n",
        "        \n",
        "        #train acc\n",
        "        _, predicted = torch.max(yhat_train.data, 1)  #returns max value, indices\n",
        "        train_total += y_train.size(0)  #keep track of total\n",
        "        train_correct += (predicted == y_train).sum().item()  #.item() give the raw number\n",
        "        train_acc = 100 * (train_correct / train_total)\n",
        "        \n",
        "        #print(y_train.shape, y_train.dtype)\n",
        "        \n",
        "        train_loss = criterion(yhat_train, y_train)\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "\n",
        "        #val accuracy\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val = X_val.float().to(device)\n",
        "            y_val = y_val.type(torch.LongTensor).to(device)\n",
        "            yhat_val  = model(X_val)\n",
        "            val_loss     = criterion(yhat_val, y_val)\n",
        "            _, predicted = torch.max(yhat_val.data, 1)  #returns max value, indices\n",
        "            val_total += y_val.size(0)  #keep track of total\n",
        "            val_correct += (predicted == y_val).sum().item()  #.item() give the raw number\n",
        "            val_acc = 100 * (val_correct / val_total)\n",
        "\n",
        "        #save the best model\n",
        "        if val_loss < best_valid_loss:\n",
        "            best_valid_loss = val_loss\n",
        "            #print(\"Model:{} saved.\".format(type(model).__name__))\n",
        "            torch.save(model.state_dict(), './models/CNN1D.pt')\n",
        "            best_model_index = i\n",
        "\n",
        "          #for plotting\n",
        "        train_losses.append(train_loss.item())\n",
        "        train_accs  .append(train_acc)\n",
        "        valid_losses.append(val_loss.item())\n",
        "        valid_accs  .append(val_acc)\n",
        "\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f\"Epoch: {i:2.0f} | Train acc: {train_acc: 2.2f} | \" +\n",
        "          f\"loss: {train_loss:2.5f} | Val acc: {val_acc: 2.2f} | \" +\n",
        "          f\"loss: {val_loss:2.5f} | Time: {epoch_mins}m {epoch_secs}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhkyoUI3D1wo",
        "outputId": "eb7f14a7-1513-41e0-c8a2-3ae005f7f75f"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 | Train acc:  63.64 | loss: 0.67505 | Val acc:  50.00 | loss: 4.10706 | Time: 0m 0s\n",
            "Epoch:  1 | Train acc:  63.64 | loss: 2.05324 | Val acc:  50.00 | loss: 1.40059 | Time: 0m 0s\n",
            "Epoch:  2 | Train acc:  100.00 | loss: 0.15380 | Val acc:  50.00 | loss: 9.04791 | Time: 0m 0s\n",
            "Epoch:  3 | Train acc:  36.36 | loss: 1.10531 | Val acc:  0.00 | loss: 0.88647 | Time: 0m 0s\n",
            "Epoch:  4 | Train acc:  100.00 | loss: 0.05392 | Val acc:  50.00 | loss: 2.24376 | Time: 0m 0s\n",
            "Epoch:  5 | Train acc:  90.91 | loss: 0.17922 | Val acc:  50.00 | loss: 3.29291 | Time: 0m 0s\n",
            "Epoch:  6 | Train acc:  90.91 | loss: 0.35457 | Val acc:  50.00 | loss: 3.53350 | Time: 0m 0s\n",
            "Epoch:  7 | Train acc:  90.91 | loss: 0.31063 | Val acc:  50.00 | loss: 2.89628 | Time: 0m 0s\n",
            "Epoch:  8 | Train acc:  90.91 | loss: 0.16084 | Val acc:  50.00 | loss: 2.10049 | Time: 0m 0s\n",
            "Epoch:  9 | Train acc:  100.00 | loss: 0.03824 | Val acc:  50.00 | loss: 1.22711 | Time: 0m 0s\n",
            "Epoch: 10 | Train acc:  100.00 | loss: 0.00650 | Val acc:  50.00 | loss: 0.53340 | Time: 0m 0s\n",
            "Epoch: 11 | Train acc:  100.00 | loss: 0.00373 | Val acc:  50.00 | loss: 0.55715 | Time: 0m 0s\n",
            "Epoch: 12 | Train acc:  100.00 | loss: 0.00752 | Val acc:  50.00 | loss: 2.06596 | Time: 0m 0s\n",
            "Epoch: 13 | Train acc:  100.00 | loss: 0.01631 | Val acc:  50.00 | loss: 3.47955 | Time: 0m 0s\n",
            "Epoch: 14 | Train acc:  100.00 | loss: 0.03285 | Val acc:  50.00 | loss: 3.81799 | Time: 0m 0s\n",
            "Epoch: 15 | Train acc:  100.00 | loss: 0.04000 | Val acc:  50.00 | loss: 4.04057 | Time: 0m 0s\n",
            "Epoch: 16 | Train acc:  100.00 | loss: 0.03973 | Val acc:  50.00 | loss: 3.89957 | Time: 0m 0s\n",
            "Epoch: 17 | Train acc:  100.00 | loss: 0.02593 | Val acc:  50.00 | loss: 2.51298 | Time: 0m 0s\n",
            "Epoch: 18 | Train acc:  100.00 | loss: 0.01229 | Val acc:  50.00 | loss: 1.28235 | Time: 0m 0s\n",
            "Epoch: 19 | Train acc:  100.00 | loss: 0.00637 | Val acc:  50.00 | loss: 1.48419 | Time: 0m 0s\n",
            "Epoch: 20 | Train acc:  100.00 | loss: 0.00351 | Val acc:  100.00 | loss: 0.26470 | Time: 0m 0s\n",
            "Epoch: 21 | Train acc:  100.00 | loss: 0.00188 | Val acc:  100.00 | loss: 0.15842 | Time: 0m 0s\n",
            "Epoch: 22 | Train acc:  100.00 | loss: 0.00098 | Val acc:  100.00 | loss: 0.23709 | Time: 0m 0s\n",
            "Epoch: 23 | Train acc:  100.00 | loss: 0.00054 | Val acc:  100.00 | loss: 0.27017 | Time: 0m 0s\n",
            "Epoch: 24 | Train acc:  100.00 | loss: 0.00043 | Val acc:  50.00 | loss: 0.43873 | Time: 0m 0s\n",
            "Epoch: 25 | Train acc:  100.00 | loss: 0.00024 | Val acc:  50.00 | loss: 0.43379 | Time: 0m 0s\n",
            "Epoch: 26 | Train acc:  100.00 | loss: 0.00015 | Val acc:  50.00 | loss: 0.69335 | Time: 0m 0s\n",
            "Epoch: 27 | Train acc:  100.00 | loss: 0.00010 | Val acc:  50.00 | loss: 0.82385 | Time: 0m 0s\n",
            "Epoch: 28 | Train acc:  100.00 | loss: 0.00008 | Val acc:  50.00 | loss: 0.84388 | Time: 0m 0s\n",
            "Epoch: 29 | Train acc:  100.00 | loss: 0.00006 | Val acc:  50.00 | loss: 0.98128 | Time: 0m 0s\n",
            "Epoch: 30 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 0.93428 | Time: 0m 0s\n",
            "Epoch: 31 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 0.99923 | Time: 0m 0s\n",
            "Epoch: 32 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.16741 | Time: 0m 0s\n",
            "Epoch: 33 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.47955 | Time: 0m 0s\n",
            "Epoch: 34 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.15913 | Time: 0m 0s\n",
            "Epoch: 35 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.41735 | Time: 0m 0s\n",
            "Epoch: 36 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.49573 | Time: 0m 0s\n",
            "Epoch: 37 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.35882 | Time: 0m 0s\n",
            "Epoch: 38 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.63565 | Time: 0m 0s\n",
            "Epoch: 39 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.68747 | Time: 0m 0s\n",
            "Epoch: 40 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.66886 | Time: 0m 0s\n",
            "Epoch: 41 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.63903 | Time: 0m 0s\n",
            "Epoch: 42 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.81159 | Time: 0m 0s\n",
            "Epoch: 43 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.77891 | Time: 0m 0s\n",
            "Epoch: 44 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.74451 | Time: 0m 0s\n",
            "Epoch: 45 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.79586 | Time: 0m 0s\n",
            "Epoch: 46 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.81532 | Time: 0m 0s\n",
            "Epoch: 47 | Train acc:  100.00 | loss: 0.00006 | Val acc:  50.00 | loss: 1.82405 | Time: 0m 0s\n",
            "Epoch: 48 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.95299 | Time: 0m 0s\n",
            "Epoch: 49 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.80277 | Time: 0m 0s\n",
            "Epoch: 50 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.85675 | Time: 0m 0s\n",
            "Epoch: 51 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.84665 | Time: 0m 0s\n",
            "Epoch: 52 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.96225 | Time: 0m 0s\n",
            "Epoch: 53 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 2.05885 | Time: 0m 0s\n",
            "Epoch: 54 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.86258 | Time: 0m 0s\n",
            "Epoch: 55 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 2.02611 | Time: 0m 0s\n",
            "Epoch: 56 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.86493 | Time: 0m 0s\n",
            "Epoch: 57 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.91446 | Time: 0m 0s\n",
            "Epoch: 58 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.88244 | Time: 0m 0s\n",
            "Epoch: 59 | Train acc:  100.00 | loss: 0.00006 | Val acc:  50.00 | loss: 1.93709 | Time: 0m 0s\n",
            "Epoch: 60 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 2.00967 | Time: 0m 0s\n",
            "Epoch: 61 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.90442 | Time: 0m 0s\n",
            "Epoch: 62 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 2.01577 | Time: 0m 0s\n",
            "Epoch: 63 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.88887 | Time: 0m 0s\n",
            "Epoch: 64 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.92764 | Time: 0m 0s\n",
            "Epoch: 65 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.99496 | Time: 0m 0s\n",
            "Epoch: 66 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.90449 | Time: 0m 0s\n",
            "Epoch: 67 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 2.02914 | Time: 0m 0s\n",
            "Epoch: 68 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.86968 | Time: 0m 0s\n",
            "Epoch: 69 | Train acc:  100.00 | loss: 0.00006 | Val acc:  50.00 | loss: 2.12121 | Time: 0m 0s\n",
            "Epoch: 70 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 2.06295 | Time: 0m 0s\n",
            "Epoch: 71 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.87431 | Time: 0m 0s\n",
            "Epoch: 72 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.92036 | Time: 0m 0s\n",
            "Epoch: 73 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 2.01447 | Time: 0m 0s\n",
            "Epoch: 74 | Train acc:  100.00 | loss: 0.00006 | Val acc:  50.00 | loss: 1.87912 | Time: 0m 0s\n",
            "Epoch: 75 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.93469 | Time: 0m 0s\n",
            "Epoch: 76 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.98421 | Time: 0m 0s\n",
            "Epoch: 77 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 2.03249 | Time: 0m 0s\n",
            "Epoch: 78 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.84586 | Time: 0m 0s\n",
            "Epoch: 79 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.91287 | Time: 0m 0s\n",
            "Epoch: 80 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.90075 | Time: 0m 0s\n",
            "Epoch: 81 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 2.03691 | Time: 0m 0s\n",
            "Epoch: 82 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.97530 | Time: 0m 0s\n",
            "Epoch: 83 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 2.06761 | Time: 0m 0s\n",
            "Epoch: 84 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 2.00184 | Time: 0m 0s\n",
            "Epoch: 85 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.95132 | Time: 0m 0s\n",
            "Epoch: 86 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.87102 | Time: 0m 0s\n",
            "Epoch: 87 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.97983 | Time: 0m 0s\n",
            "Epoch: 88 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.92662 | Time: 0m 0s\n",
            "Epoch: 89 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.92399 | Time: 0m 0s\n",
            "Epoch: 90 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.93065 | Time: 0m 0s\n",
            "Epoch: 91 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.90340 | Time: 0m 0s\n",
            "Epoch: 92 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.91167 | Time: 0m 0s\n",
            "Epoch: 93 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.95328 | Time: 0m 0s\n",
            "Epoch: 94 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.87320 | Time: 0m 0s\n",
            "Epoch: 95 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.89942 | Time: 0m 0s\n",
            "Epoch: 96 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.94089 | Time: 0m 0s\n",
            "Epoch: 97 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.93557 | Time: 0m 0s\n",
            "Epoch: 98 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.78495 | Time: 0m 0s\n",
            "Epoch: 99 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.86906 | Time: 0m 0s\n",
            "Epoch: 100 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.75820 | Time: 0m 0s\n",
            "Epoch: 101 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.76438 | Time: 0m 0s\n",
            "Epoch: 102 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.88352 | Time: 0m 0s\n",
            "Epoch: 103 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 2.07640 | Time: 0m 0s\n",
            "Epoch: 104 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.93970 | Time: 0m 0s\n",
            "Epoch: 105 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.85641 | Time: 0m 0s\n",
            "Epoch: 106 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.90695 | Time: 0m 0s\n",
            "Epoch: 107 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.70499 | Time: 0m 0s\n",
            "Epoch: 108 | Train acc:  100.00 | loss: 0.00005 | Val acc:  50.00 | loss: 1.78052 | Time: 0m 0s\n",
            "Epoch: 109 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.88302 | Time: 0m 0s\n",
            "Epoch: 110 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.87794 | Time: 0m 0s\n",
            "Epoch: 111 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.91747 | Time: 0m 0s\n",
            "Epoch: 112 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.84239 | Time: 0m 0s\n",
            "Epoch: 113 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.72515 | Time: 0m 0s\n",
            "Epoch: 114 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.88971 | Time: 0m 0s\n",
            "Epoch: 115 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.78572 | Time: 0m 0s\n",
            "Epoch: 116 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.74605 | Time: 0m 0s\n",
            "Epoch: 117 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.92802 | Time: 0m 0s\n",
            "Epoch: 118 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.94864 | Time: 0m 0s\n",
            "Epoch: 119 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.89281 | Time: 0m 0s\n",
            "Epoch: 120 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.80018 | Time: 0m 0s\n",
            "Epoch: 121 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.85125 | Time: 0m 0s\n",
            "Epoch: 122 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.88569 | Time: 0m 0s\n",
            "Epoch: 123 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.85954 | Time: 0m 0s\n",
            "Epoch: 124 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.83602 | Time: 0m 0s\n",
            "Epoch: 125 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.85499 | Time: 0m 0s\n",
            "Epoch: 126 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.77403 | Time: 0m 0s\n",
            "Epoch: 127 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.83379 | Time: 0m 0s\n",
            "Epoch: 128 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.95110 | Time: 0m 0s\n",
            "Epoch: 129 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.74910 | Time: 0m 0s\n",
            "Epoch: 130 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.66208 | Time: 0m 0s\n",
            "Epoch: 131 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.82853 | Time: 0m 0s\n",
            "Epoch: 132 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.71572 | Time: 0m 0s\n",
            "Epoch: 133 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.82825 | Time: 0m 0s\n",
            "Epoch: 134 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.95719 | Time: 0m 0s\n",
            "Epoch: 135 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.92223 | Time: 0m 0s\n",
            "Epoch: 136 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.72584 | Time: 0m 0s\n",
            "Epoch: 137 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.77627 | Time: 0m 0s\n",
            "Epoch: 138 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.81705 | Time: 0m 0s\n",
            "Epoch: 139 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.78129 | Time: 0m 0s\n",
            "Epoch: 140 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.81542 | Time: 0m 0s\n",
            "Epoch: 141 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.65155 | Time: 0m 0s\n",
            "Epoch: 142 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.64493 | Time: 0m 0s\n",
            "Epoch: 143 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.71197 | Time: 0m 0s\n",
            "Epoch: 144 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.75973 | Time: 0m 0s\n",
            "Epoch: 145 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.62783 | Time: 0m 0s\n",
            "Epoch: 146 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.83798 | Time: 0m 0s\n",
            "Epoch: 147 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.62066 | Time: 0m 0s\n",
            "Epoch: 148 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.87674 | Time: 0m 0s\n",
            "Epoch: 149 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.82318 | Time: 0m 0s\n",
            "Epoch: 150 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.61286 | Time: 0m 0s\n",
            "Epoch: 151 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.67900 | Time: 0m 0s\n",
            "Epoch: 152 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.69950 | Time: 0m 0s\n",
            "Epoch: 153 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.73584 | Time: 0m 0s\n",
            "Epoch: 154 | Train acc:  100.00 | loss: 0.00004 | Val acc:  50.00 | loss: 1.67085 | Time: 0m 0s\n",
            "Epoch: 155 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.65916 | Time: 0m 0s\n",
            "Epoch: 156 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.70107 | Time: 0m 0s\n",
            "Epoch: 157 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.55954 | Time: 0m 0s\n",
            "Epoch: 158 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.69227 | Time: 0m 0s\n",
            "Epoch: 159 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.58738 | Time: 0m 0s\n",
            "Epoch: 160 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.65600 | Time: 0m 0s\n",
            "Epoch: 161 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.80327 | Time: 0m 0s\n",
            "Epoch: 162 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.55862 | Time: 0m 0s\n",
            "Epoch: 163 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.70061 | Time: 0m 0s\n",
            "Epoch: 164 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.61199 | Time: 0m 0s\n",
            "Epoch: 165 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.70096 | Time: 0m 0s\n",
            "Epoch: 166 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.74222 | Time: 0m 0s\n",
            "Epoch: 167 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.83034 | Time: 0m 0s\n",
            "Epoch: 168 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.64183 | Time: 0m 0s\n",
            "Epoch: 169 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.57308 | Time: 0m 0s\n",
            "Epoch: 170 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.71366 | Time: 0m 0s\n",
            "Epoch: 171 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.68709 | Time: 0m 0s\n",
            "Epoch: 172 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.70864 | Time: 0m 0s\n",
            "Epoch: 173 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.75060 | Time: 0m 0s\n",
            "Epoch: 174 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.72976 | Time: 0m 0s\n",
            "Epoch: 175 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.66023 | Time: 0m 0s\n",
            "Epoch: 176 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.73423 | Time: 0m 0s\n",
            "Epoch: 177 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.54792 | Time: 0m 0s\n",
            "Epoch: 178 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.54401 | Time: 0m 0s\n",
            "Epoch: 179 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.71773 | Time: 0m 0s\n",
            "Epoch: 180 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.79092 | Time: 0m 0s\n",
            "Epoch: 181 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.71882 | Time: 0m 0s\n",
            "Epoch: 182 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.55179 | Time: 0m 0s\n",
            "Epoch: 183 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.55192 | Time: 0m 0s\n",
            "Epoch: 184 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.77453 | Time: 0m 0s\n",
            "Epoch: 185 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.65217 | Time: 0m 0s\n",
            "Epoch: 186 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.58154 | Time: 0m 0s\n",
            "Epoch: 187 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.77099 | Time: 0m 0s\n",
            "Epoch: 188 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.69404 | Time: 0m 0s\n",
            "Epoch: 189 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.74121 | Time: 0m 0s\n",
            "Epoch: 190 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.53791 | Time: 0m 0s\n",
            "Epoch: 191 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.63376 | Time: 0m 0s\n",
            "Epoch: 192 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.70630 | Time: 0m 0s\n",
            "Epoch: 193 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.66300 | Time: 0m 0s\n",
            "Epoch: 194 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.54973 | Time: 0m 0s\n",
            "Epoch: 195 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.64147 | Time: 0m 0s\n",
            "Epoch: 196 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.61977 | Time: 0m 0s\n",
            "Epoch: 197 | Train acc:  100.00 | loss: 0.00003 | Val acc:  50.00 | loss: 1.67090 | Time: 0m 0s\n",
            "Epoch: 198 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.58980 | Time: 0m 0s\n",
            "Epoch: 199 | Train acc:  100.00 | loss: 0.00002 | Val acc:  50.00 | loss: 1.54359 | Time: 0m 0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaCbs-aiP-kb",
        "outputId": "858f219e-b12d-4c59-bc93-0a4a35c5e4be"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11])"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model(X_test_tensor.float().to(device))\n",
        "#testloss = criterion(yhat, y_test_tensor.type(torch.LongTensor).to(device))\n",
        "_, predicted = torch.max(yhat.data, 1)\n",
        "\n",
        "predicted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEAbteS8D1zn",
        "outputId": "6ebaff4f-7ca0-4be6-aa27-a8e6558a8c05"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax  = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(train_losses, label = 'train loss')\n",
        "ax.plot(valid_losses, label = 'valid loss')\n",
        "plt.legend()\n",
        "ax.set_xlabel('updates')\n",
        "ax.set_ylabel('loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "y0NyPhdeK52s",
        "outputId": "99859f1c-ff9e-43a1-b60b-8733c85cc202"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'loss')"
            ]
          },
          "metadata": {},
          "execution_count": 132
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G2zb7hjM5F6",
        "outputId": "02947952-2b33-4540-968c-83437e2d5f0f"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.675048291683197,\n",
              " 2.053236246109009,\n",
              " 0.15380220115184784,\n",
              " 1.1053067445755005,\n",
              " 0.053921498358249664,\n",
              " 0.17921888828277588,\n",
              " 0.35456955432891846,\n",
              " 0.31062567234039307,\n",
              " 0.1608390361070633,\n",
              " 0.03824475780129433,\n",
              " 0.006502033676952124,\n",
              " 0.0037277527153491974,\n",
              " 0.007524594198912382,\n",
              " 0.016313744708895683,\n",
              " 0.03284941986203194,\n",
              " 0.03999921679496765,\n",
              " 0.03973234072327614,\n",
              " 0.02592817135155201,\n",
              " 0.012291228398680687,\n",
              " 0.006370546296238899,\n",
              " 0.003509231610223651,\n",
              " 0.0018831894267350435,\n",
              " 0.00098489283118397,\n",
              " 0.0005350197898223996,\n",
              " 0.00042879011016339064,\n",
              " 0.0002401618694420904,\n",
              " 0.00014604837633669376,\n",
              " 0.00010007721721194685,\n",
              " 8.405201515415683e-05,\n",
              " 6.111629045335576e-05,\n",
              " 5.361771764000878e-05,\n",
              " 4.960806472809054e-05,\n",
              " 4.7050216380739585e-05,\n",
              " 4.967100903741084e-05,\n",
              " 4.1252504161093384e-05,\n",
              " 4.435031587490812e-05,\n",
              " 3.90305976907257e-05,\n",
              " 3.6311008443590254e-05,\n",
              " 4.257245745975524e-05,\n",
              " 4.959143552696332e-05,\n",
              " 4.712166264653206e-05,\n",
              " 4.8269495891872793e-05,\n",
              " 4.397969678393565e-05,\n",
              " 4.390391040942632e-05,\n",
              " 5.19520508532878e-05,\n",
              " 4.575607817969285e-05,\n",
              " 4.7608104068785906e-05,\n",
              " 5.6533132010372356e-05,\n",
              " 4.975290721631609e-05,\n",
              " 5.2861250878777355e-05,\n",
              " 5.334867455530912e-05,\n",
              " 4.1845192754408345e-05,\n",
              " 4.6102319174678996e-05,\n",
              " 5.163741661817767e-05,\n",
              " 4.762945172842592e-05,\n",
              " 4.873444413533434e-05,\n",
              " 5.097659595776349e-05,\n",
              " 4.657890895032324e-05,\n",
              " 5.042402699473314e-05,\n",
              " 5.8135774452239275e-05,\n",
              " 4.1335813875775784e-05,\n",
              " 5.2081250032642856e-05,\n",
              " 5.159378270036541e-05,\n",
              " 4.122761674807407e-05,\n",
              " 5.1973122026538476e-05,\n",
              " 4.6286164433695376e-05,\n",
              " 5.3684281738242134e-05,\n",
              " 5.107394099468365e-05,\n",
              " 4.473721855902113e-05,\n",
              " 5.533069997909479e-05,\n",
              " 5.161538865650073e-05,\n",
              " 4.7109482693485916e-05,\n",
              " 5.010991662857123e-05,\n",
              " 4.447723404155113e-05,\n",
              " 5.510307164513506e-05,\n",
              " 4.6383775043068454e-05,\n",
              " 5.2233099268050864e-05,\n",
              " 4.4769611122319475e-05,\n",
              " 4.82358991575893e-05,\n",
              " 4.8257621529046446e-05,\n",
              " 4.4228170736460015e-05,\n",
              " 4.766186611959711e-05,\n",
              " 5.0976243073819205e-05,\n",
              " 4.780258677783422e-05,\n",
              " 4.629698014468886e-05,\n",
              " 4.580977474688552e-05,\n",
              " 4.619963146978989e-05,\n",
              " 4.413060742081143e-05,\n",
              " 3.675343032227829e-05,\n",
              " 4.277658445062116e-05,\n",
              " 4.357829675427638e-05,\n",
              " 4.8116933612618595e-05,\n",
              " 3.8248537748586386e-05,\n",
              " 3.608181941672228e-05,\n",
              " 4.38707611465361e-05,\n",
              " 4.366465509519912e-05,\n",
              " 4.214825457893312e-05,\n",
              " 3.749020106624812e-05,\n",
              " 4.085921682417393e-05,\n",
              " 4.644866567105055e-05,\n",
              " 3.798854959313758e-05,\n",
              " 3.686173295136541e-05,\n",
              " 4.0891791286412627e-05,\n",
              " 4.3914082198170945e-05,\n",
              " 4.0783379517961293e-05,\n",
              " 3.968939927290194e-05,\n",
              " 3.457618004176766e-05,\n",
              " 3.273434049333446e-05,\n",
              " 4.5138087443774566e-05,\n",
              " 3.992766869487241e-05,\n",
              " 3.766360168810934e-05,\n",
              " 3.931023820769042e-05,\n",
              " 3.148845280520618e-05,\n",
              " 3.692707105074078e-05,\n",
              " 3.825940075330436e-05,\n",
              " 4.4195647205924615e-05,\n",
              " 3.211680086678825e-05,\n",
              " 3.32001582137309e-05,\n",
              " 3.499860395095311e-05,\n",
              " 2.995011891471222e-05,\n",
              " 3.474934783298522e-05,\n",
              " 3.607110193115659e-05,\n",
              " 3.6222631024429575e-05,\n",
              " 3.996027589892037e-05,\n",
              " 3.2041043596109375e-05,\n",
              " 3.4305237932130694e-05,\n",
              " 3.372026912984438e-05,\n",
              " 3.1542709621135145e-05,\n",
              " 3.199766797479242e-05,\n",
              " 2.7815713110612705e-05,\n",
              " 3.812944851233624e-05,\n",
              " 3.1781008146936074e-05,\n",
              " 2.708992724365089e-05,\n",
              " 3.896358248312026e-05,\n",
              " 3.1932755518937483e-05,\n",
              " 3.160769847454503e-05,\n",
              " 3.339525210321881e-05,\n",
              " 2.9159140467527322e-05,\n",
              " 3.495523560559377e-05,\n",
              " 3.1921870686346665e-05,\n",
              " 3.3102758607128635e-05,\n",
              " 3.194351666024886e-05,\n",
              " 2.889920688176062e-05,\n",
              " 2.9917644496890716e-05,\n",
              " 3.079513771808706e-05,\n",
              " 2.841171772161033e-05,\n",
              " 3.0545998015441e-05,\n",
              " 3.0101813536020927e-05,\n",
              " 3.425110844545998e-05,\n",
              " 3.2030235161073506e-05,\n",
              " 3.170522177242674e-05,\n",
              " 3.251790985814296e-05,\n",
              " 3.055680645047687e-05,\n",
              " 3.496617136988789e-05,\n",
              " 3.517197183100507e-05,\n",
              " 2.5475619622739032e-05,\n",
              " 2.862838482542429e-05,\n",
              " 2.9289203666849062e-05,\n",
              " 2.5638066290412098e-05,\n",
              " 2.8552633011713624e-05,\n",
              " 3.3926087780855596e-05,\n",
              " 2.936504824901931e-05,\n",
              " 2.8964159355382435e-05,\n",
              " 2.5898074454744346e-05,\n",
              " 2.7230802515987307e-05,\n",
              " 2.5703122446429916e-05,\n",
              " 2.774001950456295e-05,\n",
              " 2.5735649614944123e-05,\n",
              " 2.5594779799575917e-05,\n",
              " 3.069771992159076e-05,\n",
              " 2.620153099996969e-05,\n",
              " 1.844367216108367e-05,\n",
              " 2.677575685083866e-05,\n",
              " 2.1163277779123746e-05,\n",
              " 2.7696607503457926e-05,\n",
              " 2.7447427783044986e-05,\n",
              " 2.6212352167931385e-05,\n",
              " 2.327612673980184e-05,\n",
              " 2.5291343263234012e-05,\n",
              " 2.7707528715836816e-05,\n",
              " 3.0069360946072266e-05,\n",
              " 2.669994682946708e-05,\n",
              " 2.685150866454933e-05,\n",
              " 2.382875572948251e-05,\n",
              " 2.5356350306537934e-05,\n",
              " 2.654823401826434e-05,\n",
              " 2.6992394850822166e-05,\n",
              " 2.6060624804813415e-05,\n",
              " 2.663490522536449e-05,\n",
              " 2.5659806851763278e-05,\n",
              " 2.5930734409485012e-05,\n",
              " 2.3384449377772398e-05,\n",
              " 2.425132151984144e-05,\n",
              " 2.4879727789084427e-05,\n",
              " 2.4164619389921427e-05,\n",
              " 2.2095124222687446e-05,\n",
              " 2.3882950699771754e-05,\n",
              " 2.6331688786740415e-05,\n",
              " 2.3579579647048377e-05,\n",
              " 2.3211148800328374e-05]"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOeze6bED12x",
        "outputId": "c92e6b3d-93a9-4c36-e90e-508e0ac5860a"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = eegConv1d().to(device)\n",
        "model.load_state_dict(torch.load('./models/CNN1D.pt'))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    total   = 0\n",
        "    correct = 0\n",
        "    for X_test, y_test in test_loader:\n",
        "        X_test = X_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "        predictions   = model(X_test)\n",
        "        test_loss     = criterion(predictions, y_test.to(torch.long))\n",
        "        # .to(torch.int64)\n",
        "        _, predicted = torch.max(predictions, 1)  #returns max value, indices\n",
        "\n",
        "        total += y_test.size(0)\n",
        "        correct += (predicted == y_test).sum().item()  #.item() give the raw number\n",
        "        acc = 100 * (correct / total)\n",
        "    \n",
        "print(f\"Accuracy: {acc:2.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCssbguuhxZk",
        "outputId": "f2510956-fac4-4fd7-ffe9-661fce2847c1"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 75.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VZgHtH5ln6G0"
      },
      "execution_count": 135,
      "outputs": []
    }
  ]
}