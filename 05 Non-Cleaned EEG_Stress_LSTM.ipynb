{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guntsvzz/CP_Project/blob/main/EEG_Stress_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa7uKqb0QGc9",
        "outputId": "78dd1ca8-9f0c-4149-8b85-70f29294b67e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mne\n",
            "  Downloading mne-1.2.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne) (2.11.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.7.3)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.7/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mne) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mne) (4.64.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mne) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mne) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mne) (1.15.0)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-1.2.2\n"
          ]
        }
      ],
      "source": [
        "pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wst52ie6eNKc",
        "outputId": "2298d7d1-246f-4f23-f75a-e60d2daefdfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting components\n",
            "  Downloading components-1.2.8-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: components\n",
            "Successfully installed components-1.2.8\n"
          ]
        }
      ],
      "source": [
        "pip install components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1c3t0r-NoDI",
        "outputId": "68b67e82-f566-4c0b-e5be-dfcab2c648e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fea28458ab0>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import mne\n",
        "import pandas as pd\n",
        "pickle.format_version\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms  \n",
        "from torch.utils.data import DataLoader, Dataset  \n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# setting seed so that splitting process and training process can be reproduce\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7jGaewr-JW2"
      },
      "source": [
        "# Import File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BnYRgITQEbb",
        "outputId": "bae144a8-0f87-4b4f-fb1a-f8b2ff606014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['exp1_Ati.csv', 'exp2_Tonson.csv', 'exp3_Pyae.csv', 'exp4_Duc.csv', 'exp5_And.csv', 'exp6_Sayaka.csv', 'exp7_Suyoga.csv', 'exp8_Arti.csv', 'exp9_Amanda.csv', 'exp10_Sapna.csv', 'exp11_Data.csv', 'exp12_Jirasak.csv', 'exp13_Max.csv', 'exp14_Ayush.csv', 'exp15_Dipesh.csv', 'exp16_Shashank.csv', 'exp17_Mi.csv']\n"
          ]
        }
      ],
      "source": [
        "main_path = os.getcwd()\n",
        "all_file = os.listdir()\n",
        "file_name = list()\n",
        "for file in all_file:\n",
        "    if file[0:3] == 'exp':\n",
        "        file_name.append(file)\n",
        "file_name = sorted(file_name)\n",
        "file = file_name[8:] + file_name[0:8]\n",
        "print(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8LdChwD-PuU"
      },
      "source": [
        "# Define Channel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "InHALuhvQSk_",
        "outputId": "2865c76a-b512-4eb2-8e27-32313b9674f6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a2805bb0-623c-4592-a753-bc2cdeab0b19\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fp1</th>\n",
              "      <th>Fp2</th>\n",
              "      <th>F7</th>\n",
              "      <th>F3</th>\n",
              "      <th>F4</th>\n",
              "      <th>F8</th>\n",
              "      <th>T3</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>T4</th>\n",
              "      <th>T5</th>\n",
              "      <th>P3</th>\n",
              "      <th>P4</th>\n",
              "      <th>T6</th>\n",
              "      <th>O1</th>\n",
              "      <th>O2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-3203.250850</td>\n",
              "      <td>-7278.934989</td>\n",
              "      <td>-22030.236963</td>\n",
              "      <td>-9343.319755</td>\n",
              "      <td>-3711.417760</td>\n",
              "      <td>2255.693347</td>\n",
              "      <td>-12633.653001</td>\n",
              "      <td>3538.079982</td>\n",
              "      <td>-22537.129824</td>\n",
              "      <td>-5153.306443</td>\n",
              "      <td>-16958.246166</td>\n",
              "      <td>-12598.918390</td>\n",
              "      <td>-25105.434669</td>\n",
              "      <td>-17961.593623</td>\n",
              "      <td>-23601.229322</td>\n",
              "      <td>-12943.850511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-3201.261544</td>\n",
              "      <td>-7280.209038</td>\n",
              "      <td>-22023.464384</td>\n",
              "      <td>-9339.251738</td>\n",
              "      <td>-3717.095103</td>\n",
              "      <td>2254.843981</td>\n",
              "      <td>-12623.482957</td>\n",
              "      <td>3549.121743</td>\n",
              "      <td>-22528.434995</td>\n",
              "      <td>-5154.111106</td>\n",
              "      <td>-16954.200501</td>\n",
              "      <td>-12597.577285</td>\n",
              "      <td>-25057.289011</td>\n",
              "      <td>-17954.262251</td>\n",
              "      <td>-23595.775496</td>\n",
              "      <td>-12940.073066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-3196.210050</td>\n",
              "      <td>-7273.592922</td>\n",
              "      <td>-22024.492565</td>\n",
              "      <td>-9336.636583</td>\n",
              "      <td>-3709.696676</td>\n",
              "      <td>2258.263798</td>\n",
              "      <td>-12628.355638</td>\n",
              "      <td>3541.052764</td>\n",
              "      <td>-22536.682789</td>\n",
              "      <td>-5150.914806</td>\n",
              "      <td>-16955.631012</td>\n",
              "      <td>-12597.934913</td>\n",
              "      <td>-25113.861276</td>\n",
              "      <td>-17958.620841</td>\n",
              "      <td>-23597.206008</td>\n",
              "      <td>-12945.079857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-3200.680399</td>\n",
              "      <td>-7275.045785</td>\n",
              "      <td>-22027.152422</td>\n",
              "      <td>-9340.548139</td>\n",
              "      <td>-3711.887146</td>\n",
              "      <td>2257.839114</td>\n",
              "      <td>-12625.986353</td>\n",
              "      <td>3540.337508</td>\n",
              "      <td>-22531.653646</td>\n",
              "      <td>-5155.362803</td>\n",
              "      <td>-16966.918643</td>\n",
              "      <td>-12605.355693</td>\n",
              "      <td>-25079.171369</td>\n",
              "      <td>-17968.075629</td>\n",
              "      <td>-23611.891104</td>\n",
              "      <td>-12950.332516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-3200.434530</td>\n",
              "      <td>-7280.454907</td>\n",
              "      <td>-22023.263219</td>\n",
              "      <td>-9337.888281</td>\n",
              "      <td>-3719.866719</td>\n",
              "      <td>2250.083059</td>\n",
              "      <td>-12625.360504</td>\n",
              "      <td>3544.740801</td>\n",
              "      <td>-22535.162870</td>\n",
              "      <td>-5155.876894</td>\n",
              "      <td>-16955.586309</td>\n",
              "      <td>-12599.141908</td>\n",
              "      <td>-25085.094581</td>\n",
              "      <td>-17956.765647</td>\n",
              "      <td>-23594.814371</td>\n",
              "      <td>-12943.895214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44995</th>\n",
              "      <td>-1023.687544</td>\n",
              "      <td>-4436.709516</td>\n",
              "      <td>-24608.175410</td>\n",
              "      <td>-7672.683319</td>\n",
              "      <td>-870.712205</td>\n",
              "      <td>3996.514260</td>\n",
              "      <td>-11068.561503</td>\n",
              "      <td>2068.966874</td>\n",
              "      <td>-19891.152667</td>\n",
              "      <td>-2561.196990</td>\n",
              "      <td>-15042.232280</td>\n",
              "      <td>-10727.473882</td>\n",
              "      <td>-23593.182694</td>\n",
              "      <td>-15365.058525</td>\n",
              "      <td>-21944.339209</td>\n",
              "      <td>-10726.311591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44996</th>\n",
              "      <td>-1016.534986</td>\n",
              "      <td>-4433.557920</td>\n",
              "      <td>-24602.051032</td>\n",
              "      <td>-7667.207142</td>\n",
              "      <td>-870.868667</td>\n",
              "      <td>3997.363627</td>\n",
              "      <td>-11064.828761</td>\n",
              "      <td>2070.620903</td>\n",
              "      <td>-19898.282873</td>\n",
              "      <td>-2560.526438</td>\n",
              "      <td>-15037.672524</td>\n",
              "      <td>-10726.758626</td>\n",
              "      <td>-23615.623846</td>\n",
              "      <td>-15353.480322</td>\n",
              "      <td>-21941.701703</td>\n",
              "      <td>-10729.016153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44997</th>\n",
              "      <td>-1015.238585</td>\n",
              "      <td>-4427.813521</td>\n",
              "      <td>-24602.676881</td>\n",
              "      <td>-7664.524932</td>\n",
              "      <td>-859.826906</td>\n",
              "      <td>4002.616287</td>\n",
              "      <td>-11067.220398</td>\n",
              "      <td>2069.458612</td>\n",
              "      <td>-19891.867923</td>\n",
              "      <td>-2558.313615</td>\n",
              "      <td>-15045.048600</td>\n",
              "      <td>-10730.424312</td>\n",
              "      <td>-23618.797793</td>\n",
              "      <td>-15361.482246</td>\n",
              "      <td>-21954.531605</td>\n",
              "      <td>-10730.089036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44998</th>\n",
              "      <td>-1018.189015</td>\n",
              "      <td>-4432.820312</td>\n",
              "      <td>-24600.844038</td>\n",
              "      <td>-7668.280026</td>\n",
              "      <td>-871.963903</td>\n",
              "      <td>4003.420949</td>\n",
              "      <td>-11061.364241</td>\n",
              "      <td>2073.280760</td>\n",
              "      <td>-19889.163362</td>\n",
              "      <td>-2559.073574</td>\n",
              "      <td>-15034.945611</td>\n",
              "      <td>-10725.953964</td>\n",
              "      <td>-23582.744429</td>\n",
              "      <td>-15350.127560</td>\n",
              "      <td>-21938.706570</td>\n",
              "      <td>-10721.908298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44999</th>\n",
              "      <td>-1012.243451</td>\n",
              "      <td>-4424.617222</td>\n",
              "      <td>-24602.632177</td>\n",
              "      <td>-7664.100249</td>\n",
              "      <td>-860.475106</td>\n",
              "      <td>4003.309191</td>\n",
              "      <td>-11065.476962</td>\n",
              "      <td>2068.184563</td>\n",
              "      <td>-19896.986472</td>\n",
              "      <td>-2556.748993</td>\n",
              "      <td>-15037.873690</td>\n",
              "      <td>-10727.071551</td>\n",
              "      <td>-23631.024197</td>\n",
              "      <td>-15354.642612</td>\n",
              "      <td>-21942.439311</td>\n",
              "      <td>-10726.669219</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45000 rows × 16 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2805bb0-623c-4592-a753-bc2cdeab0b19')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a2805bb0-623c-4592-a753-bc2cdeab0b19 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a2805bb0-623c-4592-a753-bc2cdeab0b19');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "               Fp1          Fp2            F7           F3           F4  \\\n",
              "0     -3203.250850 -7278.934989 -22030.236963 -9343.319755 -3711.417760   \n",
              "1     -3201.261544 -7280.209038 -22023.464384 -9339.251738 -3717.095103   \n",
              "2     -3196.210050 -7273.592922 -22024.492565 -9336.636583 -3709.696676   \n",
              "3     -3200.680399 -7275.045785 -22027.152422 -9340.548139 -3711.887146   \n",
              "4     -3200.434530 -7280.454907 -22023.263219 -9337.888281 -3719.866719   \n",
              "...            ...          ...           ...          ...          ...   \n",
              "44995 -1023.687544 -4436.709516 -24608.175410 -7672.683319  -870.712205   \n",
              "44996 -1016.534986 -4433.557920 -24602.051032 -7667.207142  -870.868667   \n",
              "44997 -1015.238585 -4427.813521 -24602.676881 -7664.524932  -859.826906   \n",
              "44998 -1018.189015 -4432.820312 -24600.844038 -7668.280026  -871.963903   \n",
              "44999 -1012.243451 -4424.617222 -24602.632177 -7664.100249  -860.475106   \n",
              "\n",
              "                F8            T3           C3            C4           T4  \\\n",
              "0      2255.693347 -12633.653001  3538.079982 -22537.129824 -5153.306443   \n",
              "1      2254.843981 -12623.482957  3549.121743 -22528.434995 -5154.111106   \n",
              "2      2258.263798 -12628.355638  3541.052764 -22536.682789 -5150.914806   \n",
              "3      2257.839114 -12625.986353  3540.337508 -22531.653646 -5155.362803   \n",
              "4      2250.083059 -12625.360504  3544.740801 -22535.162870 -5155.876894   \n",
              "...            ...           ...          ...           ...          ...   \n",
              "44995  3996.514260 -11068.561503  2068.966874 -19891.152667 -2561.196990   \n",
              "44996  3997.363627 -11064.828761  2070.620903 -19898.282873 -2560.526438   \n",
              "44997  4002.616287 -11067.220398  2069.458612 -19891.867923 -2558.313615   \n",
              "44998  4003.420949 -11061.364241  2073.280760 -19889.163362 -2559.073574   \n",
              "44999  4003.309191 -11065.476962  2068.184563 -19896.986472 -2556.748993   \n",
              "\n",
              "                 T5            P3            P4            T6            O1  \\\n",
              "0     -16958.246166 -12598.918390 -25105.434669 -17961.593623 -23601.229322   \n",
              "1     -16954.200501 -12597.577285 -25057.289011 -17954.262251 -23595.775496   \n",
              "2     -16955.631012 -12597.934913 -25113.861276 -17958.620841 -23597.206008   \n",
              "3     -16966.918643 -12605.355693 -25079.171369 -17968.075629 -23611.891104   \n",
              "4     -16955.586309 -12599.141908 -25085.094581 -17956.765647 -23594.814371   \n",
              "...             ...           ...           ...           ...           ...   \n",
              "44995 -15042.232280 -10727.473882 -23593.182694 -15365.058525 -21944.339209   \n",
              "44996 -15037.672524 -10726.758626 -23615.623846 -15353.480322 -21941.701703   \n",
              "44997 -15045.048600 -10730.424312 -23618.797793 -15361.482246 -21954.531605   \n",
              "44998 -15034.945611 -10725.953964 -23582.744429 -15350.127560 -21938.706570   \n",
              "44999 -15037.873690 -10727.071551 -23631.024197 -15354.642612 -21942.439311   \n",
              "\n",
              "                 O2  \n",
              "0     -12943.850511  \n",
              "1     -12940.073066  \n",
              "2     -12945.079857  \n",
              "3     -12950.332516  \n",
              "4     -12943.895214  \n",
              "...             ...  \n",
              "44995 -10726.311591  \n",
              "44996 -10729.016153  \n",
              "44997 -10730.089036  \n",
              "44998 -10721.908298  \n",
              "44999 -10726.669219  \n",
              "\n",
              "[45000 rows x 16 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1 = pd.read_csv(main_path + '/' + file[0])\n",
        "df1 = df1.drop(columns=['Unnamed: 0', '0', '17', '18', '19', '20', '21', '22',\n",
        "       '23', '24', '25', '26', '27', '28', '29', '30', '31'])\n",
        "df1.columns = ['Fp1', 'Fp2', 'F7', 'F3', 'F4', 'F8', 'T3', 'C3', 'C4', 'T4', 'T5', 'P3', 'P4', 'T6', 'O1', 'O2']\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VaLsNR5fQVeT"
      },
      "outputs": [],
      "source": [
        "# Drop some columns and save it into the new file.\n",
        "for i in range(len(file)):\n",
        "    converted_signal_path = main_path + '/Converted_signal/' + 'Conv_' + file[i]\n",
        "    df = pd.read_csv(main_path + '/' + file[i])\n",
        "    df = df.drop(columns=['Unnamed: 0', '0', '17', '18', '19', '20', '21', '22',\n",
        "        '23', '24', '25', '26', '27', '28', '29', '30', '31'])\n",
        "    df.columns = ['Fp1', 'Fp2', 'F7', 'F3', 'F4', 'F8', 'T3', 'C3', 'C4', 'T4', 'T5', 'P3', 'P4', 'T6', 'O1', 'O2']\n",
        "    df = df.set_index(df.columns[0])\n",
        "    df.to_csv(converted_signal_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C-h3CINLQatU"
      },
      "outputs": [],
      "source": [
        "def df_to_raw(df):\n",
        "    ch_names = ['Fp1', 'Fp2', 'F7', 'F3', 'F4', 'F8', 'T3', 'C3', 'C4', 'T4', 'T5', 'P3', 'P4', 'T6', 'O1', 'O2']\n",
        "    ch_types = ['eeg'] * len(ch_names)\n",
        "    df = df.T\n",
        "    sampling_rate = 250 #Hz\n",
        "    info = mne.create_info(ch_names=ch_names, ch_types=ch_types, sfreq=sampling_rate)\n",
        "    raw = mne.io.RawArray(df, info)\n",
        "    raw.set_montage('standard_1020')\n",
        "    return raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lMMdhk5YV_xY"
      },
      "outputs": [],
      "source": [
        "from mne import Epochs, find_events\n",
        "\n",
        "def getEpochs(raw, event_id, tmin, tmax, picks):\n",
        "\n",
        "    #epoching\n",
        "    events = find_events(raw)\n",
        "    epochs = Epochs(raw, events=events, event_id=event_id, \n",
        "                    tmin=tmin, tmax=tmax, baseline=None, preload=True,verbose=False, picks=picks)  #8 channels\n",
        "    print('sample drop %: ', (1 - len(epochs.events)/len(events)) * 100)\n",
        "\n",
        "    return epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1xTtVbVWCKr",
        "outputId": "3a41c844-1aa1-4c3d-c1a0-b20530f7a83e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Conv_exp9_Amanda.csv',\n",
              " 'Conv_exp7_Suyoga.csv',\n",
              " 'Conv_exp5_And.csv',\n",
              " 'Conv_exp16_Shashank.csv',\n",
              " 'Conv_exp13_Max.csv',\n",
              " 'Conv_exp1_Ati.csv',\n",
              " 'Conv_exp14_Ayush.csv',\n",
              " 'Conv_exp2_Tonson.csv',\n",
              " 'Conv_exp3_Pyae.csv',\n",
              " 'Conv_exp10_Sapna.csv',\n",
              " 'Conv_exp17_Mi.csv',\n",
              " 'Conv_exp8_Arti.csv',\n",
              " 'Conv_exp6_Sayaka.csv',\n",
              " 'Conv_exp12_Jirasak.csv',\n",
              " 'Conv_exp11_Data.csv',\n",
              " 'Conv_exp4_Duc.csv',\n",
              " 'Conv_exp15_Dipesh.csv']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the name of converted data\n",
        "main_path = os.getcwd()\n",
        "all_file = os.listdir(main_path + '/Converted_signal')\n",
        "conv_list = all_file[8:] + all_file[0:8]\n",
        "conv_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GmQQw7M1bG2q"
      },
      "outputs": [],
      "source": [
        "file_path = main_path + '/Converted_signal/'\n",
        "all_data = []\n",
        "for file in conv_list:\n",
        "    path = file_path + file\n",
        "    df = pd.read_csv(path)\n",
        "    all_data.append(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2K3PMBymWF4D"
      },
      "outputs": [],
      "source": [
        "X = np.array(all_data)\n",
        "X = np.transpose(X, (0, 2, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8-gUgAA-jTs"
      },
      "source": [
        "# Import PSS10 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gKSQ4T0alV4",
        "outputId": "9bca4d28-a3cd-4178-b6fe-bd4f4d40139f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('PSS10 - Sheet1.csv')\n",
        "y = np.array(df['label'])\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGw8680y-3N5"
      },
      "source": [
        "# Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awnviaHZcdH7",
        "outputId": "da053378-4b7d-4d23-bc4a-2e7577e77205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13, 16, 45000) (4, 16, 45000) (13,) (4,)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=999)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF6zuq4phP9E",
        "outputId": "7e13e511-3fae-4390-d6ad-e5d5bda1d06e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(11, 16, 45000) (2, 16, 45000) (11,) (2,)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=999)\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BOOZBX7BhRpR"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#scale transform each channel independently\n",
        "scalers = {}\n",
        "for i in range(X_train.shape[2]):\n",
        "    scalers[i] = MinMaxScaler(feature_range=(-1, 1))\n",
        "    X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i]) \n",
        "\n",
        "for i in range(X_val.shape[2]):\n",
        "    X_val[:, :, i]   = scalers[i].transform(X_val[:, :, i])     \n",
        "    \n",
        "for i in range(X_test.shape[2]):\n",
        "    X_test[:, :, i]  = scalers[i].transform(X_test[:, :, i]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbYlRdOGhUKy",
        "outputId": "81630190-a037-4a73-f4a8-e0bda4e36a59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.1273476113363838 -0.1530318051502836 -0.008639289287877806\n",
            "-1.0 -5.70215726188583 -5.804701026562504\n",
            "1.0000000000000004 6.412578848667387 1.633039504659759\n"
          ]
        }
      ],
      "source": [
        "print(X_train.mean(), X_val.mean(), X_test.mean())\n",
        "print(X_train.min(), X_val.min(), X_test.min())\n",
        "print(X_train.max(), X_val.max(), X_test.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOl5sDXo_C_C"
      },
      "source": [
        "# Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Y1enUccbhXGK"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(X_train).to(torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train).to(torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val).to(torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val).to(torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test).to(torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test).to(torch.float32)\n",
        "\n",
        "# Cast data to dataloader for more convenience\n",
        "training_set = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "testing_set = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "validation_set = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "train_batch_size = 5\n",
        "val_batch_size = len(validation_set)\n",
        "test_batch_size = len(testing_set)\n",
        "\n",
        "train_loader = DataLoader(training_set, train_batch_size, shuffle=True)\n",
        "val_loader = DataLoader(validation_set, val_batch_size, shuffle=True)\n",
        "test_loader = DataLoader(testing_set, test_batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEJ3VJt3huYw",
        "outputId": "6a520e67-09f6-4346-aff4-890736bd9a8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 2.9449e-01,  2.9322e-01,  2.9747e-01,  ...,  1.0000e+00,\n",
            "           1.0000e+00,  1.0000e+00],\n",
            "         [ 3.0708e-01,  3.0608e-01,  3.0970e-01,  ...,  5.2585e-01,\n",
            "           5.5209e-01,  5.1636e-01],\n",
            "         [ 6.7623e-01,  6.7259e-01,  6.7672e-01,  ...,  1.0000e+00,\n",
            "           1.0000e+00,  1.0000e+00],\n",
            "         ...,\n",
            "         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
            "           1.0000e+00,  1.0000e+00],\n",
            "         [ 8.0891e-01,  8.2519e-01,  8.1632e-01,  ...,  9.8336e-01,\n",
            "           9.7279e-01,  9.7556e-01],\n",
            "         [ 1.1666e-01,  1.1489e-01,  1.1578e-01,  ...,  1.1091e-01,\n",
            "           1.0793e-01,  1.0934e-01]],\n",
            "\n",
            "        [[ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  5.2233e-01,\n",
            "           5.2358e-01,  5.2387e-01],\n",
            "         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  6.5781e-01,\n",
            "           6.8749e-01,  6.4978e-01],\n",
            "         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  7.5530e-01,\n",
            "           7.5613e-01,  7.5630e-01],\n",
            "         ...,\n",
            "         [ 1.1717e-01,  1.2693e-01,  1.1979e-01,  ...,  7.3270e-01,\n",
            "           7.4560e-01,  7.2589e-01],\n",
            "         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  7.7411e-01,\n",
            "           7.7420e-01,  7.7631e-01],\n",
            "         [-5.7872e-01, -5.7725e-01, -5.7837e-01,  ..., -3.7240e-01,\n",
            "          -3.7023e-01, -3.7358e-01]],\n",
            "\n",
            "        [[-3.5848e-01, -3.6159e-01, -3.6451e-01,  ..., -4.3825e-01,\n",
            "          -4.3881e-01, -4.3796e-01],\n",
            "         [-2.6421e-01, -2.6681e-01, -2.6947e-01,  ..., -4.3212e-01,\n",
            "          -4.2254e-01, -4.3473e-01],\n",
            "         [-2.2465e-01, -2.3675e-01, -2.3012e-01,  ..., -3.6256e-01,\n",
            "          -3.6286e-01, -3.6190e-01],\n",
            "         ...,\n",
            "         [-6.0080e-01, -6.0145e-01, -6.0161e-01,  ..., -2.0152e-01,\n",
            "          -2.0056e-01, -2.0242e-01],\n",
            "         [ 2.0265e-02,  2.0469e-02,  1.9499e-02,  ...,  1.2127e-01,\n",
            "           1.2086e-01,  1.2131e-01],\n",
            "         [ 4.8973e-02,  4.4209e-02,  4.3238e-02,  ...,  1.0493e-01,\n",
            "           1.0376e-01,  1.0937e-01]],\n",
            "\n",
            "        [[ 2.5658e-01,  2.5487e-01,  2.5507e-01,  ..., -8.9398e-02,\n",
            "          -8.9928e-02, -8.8101e-02],\n",
            "         [ 4.0647e-01,  4.0533e-01,  4.0510e-01,  ...,  3.4936e-03,\n",
            "           2.0737e-02, -1.2799e-03],\n",
            "         [ 6.8780e-02,  6.0319e-02,  6.6661e-02,  ..., -4.3747e-03,\n",
            "          -4.3962e-03, -3.7484e-03],\n",
            "         ...,\n",
            "         [ 3.8286e-01,  3.8092e-01,  3.8167e-01,  ...,  8.8755e-01,\n",
            "           8.8878e-01,  8.8807e-01],\n",
            "         [ 5.9742e-01,  5.9858e-01,  5.9696e-01,  ...,  4.6354e-01,\n",
            "           4.6332e-01,  4.6498e-01],\n",
            "         [ 3.7177e-01,  3.7049e-01,  3.6908e-01,  ...,  2.0694e-01,\n",
            "           2.0385e-01,  2.0626e-01]],\n",
            "\n",
            "        [[-5.2427e-01, -5.2510e-01, -5.2529e-01,  ..., -6.2139e-01,\n",
            "          -6.2147e-01, -6.2065e-01],\n",
            "         [-8.2488e-01, -8.2498e-01, -8.2534e-01,  ..., -9.2147e-01,\n",
            "          -9.2005e-01, -9.2193e-01],\n",
            "         [ 6.1787e-01,  6.1359e-01,  6.1631e-01,  ...,  4.6422e-01,\n",
            "           4.6379e-01,  4.6520e-01],\n",
            "         ...,\n",
            "         [-2.3044e-01, -2.3137e-01, -2.3107e-01,  ...,  1.8161e-01,\n",
            "           1.8238e-01,  1.8175e-01],\n",
            "         [ 3.4564e-01,  3.4604e-01,  3.4490e-01,  ...,  2.5742e-01,\n",
            "           2.5660e-01,  2.5811e-01],\n",
            "         [ 1.4900e-01,  1.4751e-01,  1.4650e-01,  ...,  6.3942e-04,\n",
            "          -2.5215e-03, -2.8354e-04]]])\n"
          ]
        }
      ],
      "source": [
        "for data, label in train_loader:\n",
        "  print(data)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVMFMBwAjImF",
        "outputId": "44e6a7c3-faab-4ba6-8ab9-4d5482b2bd0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 16, 45000])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HaiKzvgCjc3_"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jHB8Zgs_kpm6"
      },
      "outputs": [],
      "source": [
        "lstm_model = nn.LSTM(input_size=45000, hidden_size=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMV89z27_HmH"
      },
      "source": [
        "# LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "U0bk5XjGjoMk"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    '''\n",
        "    Expected Input Shape: (batch, channels, seq_len, )\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=num_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * num_layers, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, channels, seq len, ]\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        \n",
        "        #out = [batch size, seq len, hidden dim * num directions]        \n",
        "        #hn = [num layers * num directions, batch size, hidden dim]\n",
        "        #cn = [num layers * num directions, batch size, hidden dim]\n",
        "        \n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
        "        #hn = [batch size, hidden dim * num directions]\n",
        "        \n",
        "        return self.fc(hn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TIOLWwwXpMpw"
      },
      "outputs": [],
      "source": [
        "#explicitly initialize weights for better learning\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear):   #if layer is of Linear\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LSTM):   #if layer is of LSTM\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.orthogonal_(param)  #orthogonal is a common way to initialize weights for RNN/LSTM/GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khjrPaLwxtfq",
        "outputId": "65e6e30b-d77c-4a1f-a5c3-6089e1952e6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (lstm): LSTM(45000, 256, num_layers=2, batch_first=True, dropout=0.7, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_dim     = 45000 \n",
        "hidden_dim    = 256 \n",
        "num_layers    = 2  \n",
        "output_dim    = 1  #we got 2 classes so we can output only 1 number, 0 for first class and 1 for another class\n",
        "bidirectional = True  #uses bidirectional LSTM\n",
        "dropout       = 0.7 \n",
        "\n",
        "#define the model\n",
        "model = LSTM(input_dim, hidden_dim, num_layers, output_dim, bidirectional, dropout)\n",
        "\n",
        "#send to cuda so we can use the GPU\n",
        "model = model.to(device)  \n",
        "\n",
        "#apply weights\n",
        "model.apply(initialize_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaDuk4jtyDkT",
        "outputId": "29b46bbd-30e0-4e5c-b0fd-4e1b5aa958cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model LSTM has 94,265,857 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model {type(model).__name__} has {count_parameters(model):,} trainable parameters')# Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vdx6Dms0yGL7"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "lr = 0.0001\n",
        "\n",
        "#can also try SGD which should not make too much of a difference\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr) \n",
        "\n",
        "#combine sigmoid with binary entropy; binary entropy is used for binary classification\n",
        "criterion = nn.BCEWithLogitsLoss() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BV4d52uyyYlE"
      },
      "outputs": [],
      "source": [
        "#for measuring accuracy\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMJk3mnr2xOH",
        "outputId": "82a070fd-c3dc-4276-e1bf-46f96b8f6f82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.2461],\n",
              "        [0.1433]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.sigmoid(torch.tensor([[-1.1193],\n",
        "        [-1.7878]]) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "lqDIAbvRylTR"
      },
      "outputs": [],
      "source": [
        "#for timing the epochs\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "USKSyPIIyhdj"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(model, train_loader,  optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "    epoch_train_loss = 0\n",
        "    epoch_train_acc  = 0\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "    \n",
        "        # data shape: (batch, channel, seq len)\n",
        "        data  = batch[0].to(device)  \n",
        "        # data shape: (batch, seq len, channel)\n",
        "\n",
        "        # label shape: (batch, 1)\n",
        "        label = batch[1].to(device).reshape(-1, 1)\n",
        "        \n",
        "        #predict\n",
        "        output = model(data).reshape(-1, 1)  #output shape: (batch, 1)\n",
        "        loss   = criterion(output, label)\n",
        "        \n",
        "        #backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #for visualizing\n",
        "        epoch_train_loss += loss.item()\n",
        "        acc = binary_accuracy(output, label)\n",
        "        epoch_train_acc += acc.item()\n",
        "        \n",
        "    epoch_train_loss = epoch_train_loss / len(train_loader)\n",
        "    epoch_train_acc  = epoch_train_acc  / len(train_loader)\n",
        "    \n",
        "    return epoch_train_loss, epoch_train_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DxaF5O8YymHK"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, val_loader, criterion):\n",
        "\n",
        "    model.eval()\n",
        "    epoch_val_loss = 0\n",
        "    epoch_val_acc  = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            \n",
        "            # data shape: (batch, channel, seq len)\n",
        "            data  = batch[0].to(device) \n",
        "            label = batch[1].to(device).reshape(-1, 1)\n",
        "            \n",
        "            #predict\n",
        "            output = model(data).reshape(-1, 1)\n",
        "            loss   = criterion(output, label)\n",
        "            \n",
        "            #for visualizing\n",
        "            epoch_val_loss += loss.item()\n",
        "            acc = binary_accuracy(output, label)\n",
        "            epoch_val_acc += acc.item()\n",
        "    \n",
        "    epoch_val_loss =  epoch_val_loss / len(val_loader)\n",
        "    epoch_val_acc  =  epoch_val_acc  / len(val_loader)\n",
        "    \n",
        "    return epoch_val_loss, epoch_val_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "EDCgEWqTyrci"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfX4uH96yoCI",
        "outputId": "45d4faaa-c85d-420d-84d5-1a09cfea5228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\t Train Loss: 0.772 | Train Acc: 26.67%\n",
            "\t Val. Loss: 0.721  |  Val. Acc: 0.00%\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.653 | Train Acc: 66.67%\n",
            "\t Val. Loss: 0.807  |  Val. Acc: 50.00%\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.547 | Train Acc: 80.00%\n",
            "\t Val. Loss: 1.040  |  Val. Acc: 0.00%\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.547 | Train Acc: 80.00%\n",
            "\t Val. Loss: 1.290  |  Val. Acc: 0.00%\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.397 | Train Acc: 86.67%\n",
            "\t Val. Loss: 1.559  |  Val. Acc: 0.00%\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.475 | Train Acc: 86.67%\n",
            "\t Val. Loss: 1.795  |  Val. Acc: 0.00%\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.398 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.017  |  Val. Acc: 0.00%\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.443 | Train Acc: 80.00%\n",
            "\t Val. Loss: 2.187  |  Val. Acc: 0.00%\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.357 | Train Acc: 80.00%\n",
            "\t Val. Loss: 2.336  |  Val. Acc: 0.00%\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.339 | Train Acc: 80.00%\n",
            "\t Val. Loss: 2.433  |  Val. Acc: 0.00%\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.376 | Train Acc: 80.00%\n",
            "\t Val. Loss: 2.519  |  Val. Acc: 0.00%\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.469 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.509  |  Val. Acc: 0.00%\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.473 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.467  |  Val. Acc: 0.00%\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.478 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.446  |  Val. Acc: 0.00%\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.419 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.342  |  Val. Acc: 0.00%\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.395 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.245  |  Val. Acc: 0.00%\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.344 | Train Acc: 80.00%\n",
            "\t Val. Loss: 2.165  |  Val. Acc: 0.00%\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.427 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.156  |  Val. Acc: 0.00%\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.292 | Train Acc: 93.33%\n",
            "\t Val. Loss: 2.194  |  Val. Acc: 0.00%\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.594 | Train Acc: 60.00%\n",
            "\t Val. Loss: 2.203  |  Val. Acc: 0.00%\n",
            "Epoch: 21 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.304 | Train Acc: 80.00%\n",
            "\t Val. Loss: 2.164  |  Val. Acc: 0.00%\n",
            "Epoch: 22 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.298 | Train Acc: 93.33%\n",
            "\t Val. Loss: 2.144  |  Val. Acc: 0.00%\n",
            "Epoch: 23 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.273 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.188  |  Val. Acc: 0.00%\n",
            "Epoch: 24 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.438 | Train Acc: 93.33%\n",
            "\t Val. Loss: 2.261  |  Val. Acc: 0.00%\n",
            "Epoch: 25 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.329 | Train Acc: 80.00%\n",
            "\t Val. Loss: 2.323  |  Val. Acc: 0.00%\n",
            "Epoch: 26 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.473 | Train Acc: 53.33%\n",
            "\t Val. Loss: 2.319  |  Val. Acc: 0.00%\n",
            "Epoch: 27 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.319 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.300  |  Val. Acc: 0.00%\n",
            "Epoch: 28 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.387 | Train Acc: 93.33%\n",
            "\t Val. Loss: 2.216  |  Val. Acc: 0.00%\n",
            "Epoch: 29 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.326 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.186  |  Val. Acc: 0.00%\n",
            "Epoch: 30 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.517 | Train Acc: 60.00%\n",
            "\t Val. Loss: 2.152  |  Val. Acc: 0.00%\n",
            "Epoch: 31 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.310 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.102  |  Val. Acc: 0.00%\n",
            "Epoch: 32 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.277 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.107  |  Val. Acc: 0.00%\n",
            "Epoch: 33 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.489 | Train Acc: 60.00%\n",
            "\t Val. Loss: 2.098  |  Val. Acc: 0.00%\n",
            "Epoch: 34 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.556 | Train Acc: 60.00%\n",
            "\t Val. Loss: 2.040  |  Val. Acc: 0.00%\n",
            "Epoch: 35 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.386 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.046  |  Val. Acc: 0.00%\n",
            "Epoch: 36 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.409 | Train Acc: 66.67%\n",
            "\t Val. Loss: 2.124  |  Val. Acc: 0.00%\n",
            "Epoch: 37 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.311 | Train Acc: 80.00%\n",
            "\t Val. Loss: 2.282  |  Val. Acc: 0.00%\n",
            "Epoch: 38 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.421 | Train Acc: 53.33%\n",
            "\t Val. Loss: 2.507  |  Val. Acc: 0.00%\n",
            "Epoch: 39 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.299 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.714  |  Val. Acc: 0.00%\n",
            "Epoch: 40 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.360 | Train Acc: 86.67%\n",
            "\t Val. Loss: 2.883  |  Val. Acc: 0.00%\n",
            "Epoch: 41 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.241 | Train Acc: 86.67%\n",
            "\t Val. Loss: 3.058  |  Val. Acc: 0.00%\n",
            "Epoch: 42 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.278 | Train Acc: 86.67%\n",
            "\t Val. Loss: 3.188  |  Val. Acc: 0.00%\n",
            "Epoch: 43 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.212 | Train Acc: 86.67%\n",
            "\t Val. Loss: 3.243  |  Val. Acc: 0.00%\n",
            "Epoch: 44 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.213 | Train Acc: 86.67%\n",
            "\t Val. Loss: 3.294  |  Val. Acc: 0.00%\n",
            "Epoch: 45 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.252 | Train Acc: 86.67%\n",
            "\t Val. Loss: 3.290  |  Val. Acc: 0.00%\n",
            "Epoch: 46 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.210 | Train Acc: 86.67%\n",
            "\t Val. Loss: 3.322  |  Val. Acc: 0.00%\n",
            "Epoch: 47 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.209 | Train Acc: 86.67%\n",
            "\t Val. Loss: 3.368  |  Val. Acc: 0.00%\n",
            "Epoch: 48 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.204 | Train Acc: 93.33%\n",
            "\t Val. Loss: 3.427  |  Val. Acc: 0.00%\n",
            "Epoch: 49 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.178 | Train Acc: 93.33%\n",
            "\t Val. Loss: 3.461  |  Val. Acc: 0.00%\n",
            "Epoch: 50 | Epoch Time: 0m 0s\n",
            "\t Train Loss: 0.188 | Train Acc: 93.33%\n",
            "\t Val. Loss: 3.472  |  Val. Acc: 0.00%\n"
          ]
        }
      ],
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_losses = []\n",
        "train_accs   = []\n",
        "valid_losses = []\n",
        "valid_accs   = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, val_loader, criterion)\n",
        "    \n",
        "    #for plotting\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs  .append(train_acc)\n",
        "    valid_losses.append(valid_loss)\n",
        "    valid_accs  .append(valid_acc)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), './models/DEAP_BiLSTM.pt')\n",
        "        \n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f}  |  Val. Acc: {valid_acc*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOaNhT9B1zxT",
        "outputId": "cd1e86d3-4c59-49b6-ee1b-0f24997781be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.2666666756073634,\n",
              " 0.6666666666666666,\n",
              " 0.800000011920929,\n",
              " 0.800000011920929,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.800000011920929,\n",
              " 0.800000011920929,\n",
              " 0.800000011920929,\n",
              " 0.800000011920929,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.800000011920929,\n",
              " 0.8666666746139526,\n",
              " 0.9333333373069763,\n",
              " 0.600000003973643,\n",
              " 0.800000011920929,\n",
              " 0.9333333373069763,\n",
              " 0.8666666746139526,\n",
              " 0.9333333373069763,\n",
              " 0.800000011920929,\n",
              " 0.5333333412806193,\n",
              " 0.8666666746139526,\n",
              " 0.9333333373069763,\n",
              " 0.8666666746139526,\n",
              " 0.600000003973643,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.600000003973643,\n",
              " 0.600000003973643,\n",
              " 0.8666666746139526,\n",
              " 0.6666666666666666,\n",
              " 0.800000011920929,\n",
              " 0.5333333412806193,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.8666666746139526,\n",
              " 0.9333333373069763,\n",
              " 0.9333333373069763,\n",
              " 0.9333333373069763]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "WfoWbyWnyp_K",
        "outputId": "36047099-c3a1-4e3b-8ed0-c86d934383d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'loss')"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax  = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(train_losses, label = 'train loss')\n",
        "ax.plot(valid_losses, label = 'valid loss')\n",
        "plt.legend()\n",
        "ax.set_xlabel('updates')\n",
        "ax.set_ylabel('loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "mAgd8-Pt35gu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPf0a5Cj/Ax1H7U/tpFr93N",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
